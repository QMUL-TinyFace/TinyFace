<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<!-- saved from url=(0055)https://qmul-survface.github.io/QMUL-SurvFace/index.htm -->
<html xmlns="http://www.w3.org/1999/xhtml">
   <head>
      <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
      <title>QMUL-TinyFace</title>
      <meta name="description" content="">
      <meta name="keywords" content="TinyFace; 
                                     QMUL Native Low-resolution Face Recognition Challenge;                                                                          benchmark; computer vision;">
      <!-- Fonts and stuff -->
      <link href="./QMUL-TinyFace_files/css" rel="stylesheet" type="text/css">
      <link rel="stylesheet" type="text/css" href="./QMUL-SurvFace_files/project.css" media="screen">
      <link rel="stylesheet" type="text/css" media="screen" href="./QMUL-SurvFace_files/iconize.css">
      <link href="./QMUL-TinyFace_files/bootstrap.css" rel="stylesheet">	  <link href="./QMUL-TinyFace_files/bootstrap-responsive.css" rel="stylesheet">
      <link rel="shortcut icon" href="./images/new_icon.png">
      <script type="text/javascript" async="" src="./QMUL-TinyFace_files/ga.js"></script>
      		<link rel="apple-touch-icon-precomposed" sizes="144x144" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-144-precomposed.png">        <link rel="apple-touch-icon-precomposed" sizes="114x114" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-114-precomposed.png">        <link rel="apple-touch-icon-precomposed" sizes="72x72" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-72-precomposed.png">        <link rel="apple-touch-icon-precomposed" href="http://twitter.github.com/bootstrap/assets/ico/apple-touch-icon-57-precomposed.png">
        
   </head>
   <body>
      <div id="content">
         <div id="content-inner">
            <div class="section head">
               <p>
               <h1 id="tinyface">TinyFace: Native Unconstrained Low-resolution Face Dataset</h1>
               </p>
               <div class="affiliations">
                  <p><a href="http://www.eecs.qmul.ac.uk/~zc302/">Zhiyi Cheng</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~xiatian/">Xiatian Zhu</a> &nbsp; &nbsp; &nbsp; &nbsp;
                     <a href="http://www.eecs.qmul.ac.uk/~sgg/">Shaogang Gong</a>
                  <p><a href="http://vision.eecs.qmul.ac.uk/">Computer Vision Group,  </a >
                     <a href="http://www.eecs.qmul.ac.uk/">School of Electronic Engineering and Computer Science,  </a >
                     <a href="http://www.qmul.ac.uk/">Queen Mary University of London</a >
               </div>
               <ul id="tabs">
                  <li><a href="https://qmul-survface.github.io/index.html" name="#tab1" id="current">Home</a></li>
                  <li><a href="https://qmul-survface.github.io/protocols.html" name="#tab2">Protocols</a></li>
                  <li><a href="https://qmul-survface.github.io/benchmark.html" name="#tab3">Benchmark</a></li>
               </ul>
            </div>
            <center><img style="width: 80%;" src="./images/datasets_visualisation.png" alt="QMUL SurvFace"></center>
            <h2 id="description">Description</h2>
            <p>We create a large scale face recognition benchmark, named TinyFace, to facilitate the investigation of natively LRFR at large scales (large gallery population sizes) in deep learning. The TinyFace dataset consists of 5,139 labelled facial identities given by 169,403 native LR face images (average 20Ã—16 pixels) designed for 1:N recognition test. All the LR faces in TinyFace are collected from public web data across a large variety of imaging scenarios, captured under uncontrolled viewing conditions in pose, illumination, occlusion and background. Beyond artificially down-sampling HR face images for LRFR performance test as in previous works, to our best knowledge, this is the first systematic study focusing specially on face recognition of native LR images.
            </p>
            <h2 id="news">News</h2>
            <p>
            <ul>
               <li><strong>October 03, 2018:</strong> TinyFace dataset, the evaluation protocol, and test codes are released. </li>
            </ul>
            </p>
            <h2 id="download">Download</h2>
            <p>QMUL-SurvFace Dataset and Evaluation Codes: 
               <!--[<a href="https://drive.google.com/open?id=13ch6BPaexlKt8gXB_I8aX7p1G3yPm2Bl" target="_blank">Google Drive</a>] 
               [<a href="https://pan.baidu.com/s/1O55042SMxLYqBWdGGQ_4_g" target="_blank">Baidu Cloud</a>]-->
            </p>
            <!--<h2 id="citation">Citation</h2>
            <pre>
Zhiyi Cheng, Xiatian Zhu and Shaogang Gong.
<em>Surveillance Face Recognition Challenge</em>.
arXiv:1804.09691, 2018. <a href="QMUL-SurvFace_bibtex.html">Bibtex</a> <a href="https://arxiv.org/pdf/1804.09691.pdf">Paper</a>
</pre>-->
            <!--<h2 id="Related Datasets">Related Datasets</h2>
            <p>We list below existing surveillance face recognition datasets. More extensive comparisons of face recognition datasets can be found in the <a href="https://arxiv.org/pdf/1804.09691.pdf">paper.</a></p>
            <ul>
               <li><a href="http://vast.uccs.edu/Opensetface/">UCCS Challenge</a>:  UCCS is a high-resolution surveillance face detection and recognition challenge. 
                  It contains 1,732 identities captured by a Canon 7D camera fitted with Sigma 800mm F5.6 EX APO DG HSM lens.
               </li>
               <li><a href="http://www.scface.org/">SCface Dataset</a>: SCface is one of the earliest surveillance face recognition datasets. 
                  It consists of 4,160 static images (in visible and infrared spectrum) of 130 identities.
               </li>
            </ul>-->
            <!--<h2 id="Licence">Licence</h2>
            <p>Please notice that the QMUL-SurvFace challenge is made available for academic research purpose only. 
               All the images were collected from the existing person re-identification datasets, 
               and the copyright belongs to the original owners. 
            </p>-->
            <h2 id="contact">Contact</h2>
            <p>Please feel free to send any questions and/or comments to Zhiyi Cheng at <strong>z.cheng@qmul.ac.uk</strong></p>
            <br>
            <br>
         
      
		    		
	</div>	</div>
   </body>
</html>

